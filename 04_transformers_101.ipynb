{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XneHhTu2j9bl"
      },
      "source": [
        "# Lab 04: Transformers 101\n",
        "\n",
        "**AI Demystified: Decoding Models, Compute, and Connectivity**\n",
        "\n",
        "Welcome! This lab gives a gentle, hands-on tour of the Hugging Face **Transformers** library.\n",
        "\n",
        "**Goals**\n",
        "\n",
        "**You will see:**\n",
        "- How tokenization turns text into tokens and integer IDs\n",
        "- What GPT‑2 expects as model inputs (tensor shapes)\n",
        "- Token-level embeddings from GPT‑2 (last hidden state)\n",
        "- A tiny demo of summarization and sentiment classification\n",
        "\n",
        "---"
      ],
      "id": "XneHhTu2j9bl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A60NQ96rj9bs"
      },
      "source": [
        "## Step 0: Tokenization concepts\n",
        "\n",
        "- **Tokenization** splits text into small units (tokens). Models work on token IDs, not raw text.\n",
        "- GPT‑2 uses a **byte-level BPE** tokenizer. It marks a **leading space** before a token with the special character **`Ġ`** (U+0120).\n",
        "  - Example: `\" power\"` → token string `\"Ġpower\"` (space + \"power\").\n",
        "  - This keeps whitespace information without using a separate \"space\" token.\n",
        "- When you **decode** IDs back to text, the `Ġ` markers disappear and you recover the original spacing.\n",
        "\n",
        "> Below, we’ll print tokens and IDs, then verify we can decode back to the original sentence."
      ],
      "id": "A60NQ96rj9bs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuxX0R0Tj9bt"
      },
      "source": [
        "## Step 1: Install dependencies"
      ],
      "id": "UuxX0R0Tj9bt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install"
      },
      "source": [
        "%%capture\n",
        "!pip -q install \"transformers>=4.41\" torch --extra-index-url https://download.pytorch.org/whl/cpu"
      ],
      "id": "install",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AckrK6z7j9bw"
      },
      "source": [
        "## Step 2: Imports"
      ],
      "id": "AckrK6z7j9bw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "# Core Transformers imports. torch is the tensor engine used under the hood.\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "import torch"
      ],
      "id": "imports",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtbu1ng1j9bx"
      },
      "source": [
        "## Step 3: Tokenization (GPT‑2)\n",
        "We’ll tokenize a short sentence, view tokens and IDs, then do a decode round‑trip."
      ],
      "id": "qtbu1ng1j9bx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tokenization"
      },
      "source": [
        "# Load the GPT‑2 tokenizer.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "id": "tokenization",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A small example sentence.\n",
        "text = \"Transformers are powerful models for language tasks.\""
      ],
      "metadata": {
        "id": "aMDEWUM0k3Gq"
      },
      "id": "aMDEWUM0k3Gq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "pLJthI-zk61Y"
      },
      "id": "pLJthI-zk61Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Token strings (note the 'Ġ' marker indicating a leading space before tokens after the first).\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "mqe4Byc0lCwM"
      },
      "id": "mqe4Byc0lCwM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokens))"
      ],
      "metadata": {
        "id": "Yrw9MkyllGzJ"
      },
      "id": "Yrw9MkyllGzJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Token IDs are the integer form models actually consume.\n",
        "ids = tokenizer.encode(text)"
      ],
      "metadata": {
        "id": "yqQdKbCpld1R"
      },
      "id": "yqQdKbCpld1R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ids)"
      ],
      "metadata": {
        "id": "t4AIDAIjlmKL"
      },
      "id": "t4AIDAIjlmKL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ids))"
      ],
      "metadata": {
        "id": "EBjayRxUlqpT"
      },
      "id": "EBjayRxUlqpT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode round‑trip: IDs → text (Ġ markers are not visible in decoded text).\n",
        "decoded = tokenizer.decode(ids)"
      ],
      "metadata": {
        "id": "sEgcICT8ltBd"
      },
      "id": "sEgcICT8ltBd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoded)"
      ],
      "metadata": {
        "id": "lxfTYnUqlwsF"
      },
      "id": "lxfTYnUqlwsF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45_SU4Ioj9bz"
      },
      "source": [
        "## Step 4: Build model‑ready tensors\n",
        "We create the **PyTorch tensors** (`input_ids`, `attention_mask`) that GPT‑2 expects. We only inspect shapes here."
      ],
      "id": "45_SU4Ioj9bz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tensors"
      },
      "source": [
        "# The convenient call form builds a full batch with tensors.\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "print(inputs[\"input_ids\"].shape)"
      ],
      "id": "tensors",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DccSyX8Fj9b0"
      },
      "source": [
        "## Step 5: GPT‑2 embeddings (last hidden state)\n",
        "We run the base GPT‑2 model to get **contextual token embeddings**. Each token gets a vector influenced by its context."
      ],
      "id": "DccSyX8Fj9b0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "embeddings"
      },
      "source": [
        "# Load the base GPT‑2 transformer (no language model head).\n",
        "model = AutoModel.from_pretrained(\"gpt2\")"
      ],
      "id": "embeddings",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference only: no gradient tracking needed.\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)"
      ],
      "metadata": {
        "id": "AOtDB7x0mHqO"
      },
      "id": "AOtDB7x0mHqO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Token-level embeddings after the final transformer layer.\n",
        "last_hidden = outputs.last_hidden_state  # [batch, seq_len, hidden_size]\n",
        "print(last_hidden.shape)"
      ],
      "metadata": {
        "id": "Vm0dKz1MmLgT"
      },
      "id": "Vm0dKz1MmLgT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Peek at the first 10 dimensions of the first token's vector.\n",
        "print(last_hidden[0, 0, :10])"
      ],
      "metadata": {
        "id": "s9ONb6VymirU"
      },
      "id": "s9ONb6VymirU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input embedding matrix (lookup table BEFORE transformer layers).\n",
        "E = model.get_input_embeddings().weight  # [vocab_size, hidden_size]\n",
        "print(E.shape)"
      ],
      "metadata": {
        "id": "ODfPJ6bImrbT"
      },
      "id": "ODfPJ6bImrbT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbpM-tvgj9b1"
      },
      "source": [
        "## Step 6: Summarization (compact model)\n",
        "We use a small summarization model for quick demos. This is independent of GPT‑2."
      ],
      "id": "vbpM-tvgj9b1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "summarization"
      },
      "source": [
        "# A short paragraph to summarize.\n",
        "long_text = (\n",
        "    \"Cisco HyperFabric integrates servers, networking, and GPUs for high-performance AI workloads. \"\n",
        "    \"It uses RoCEv2 with PFC/ECN to maintain low loss and predictable latency across a leaf-spine fabric, \"\n",
        "    \"improving collective operations and training throughput.\"\n",
        "    )"
      ],
      "id": "summarization",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DistilBART CNN is smaller than bart-large and runs quickly on CPU.\n",
        "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")"
      ],
      "metadata": {
        "id": "eA_lzJIEm8Hy"
      },
      "id": "eA_lzJIEm8Hy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = summarizer(long_text, max_length=40, min_length=12, do_sample=False)"
      ],
      "metadata": {
        "id": "gy3I5SwgnD22"
      },
      "id": "gy3I5SwgnD22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(summary[0][\"summary_text\"])"
      ],
      "metadata": {
        "id": "bBOLWQvynUg2"
      },
      "id": "bBOLWQvynUg2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExuaLf32j9b2"
      },
      "source": [
        "## Step 7: Text classification (sentiment)\n",
        "A quick look at a ready‑to‑use sentiment pipeline."
      ],
      "id": "ExuaLf32j9b2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "classification"
      },
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")"
      ],
      "id": "classification",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classifier(\"I love working with AI infrastructure!\"))"
      ],
      "metadata": {
        "id": "6yVuDvpvnmGl"
      },
      "id": "6yVuDvpvnmGl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classifier(\"This restaurant service is frustrating and unreliable.\"))"
      ],
      "metadata": {
        "id": "IyqRGj41nqLM"
      },
      "id": "IyqRGj41nqLM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-xGvjWaj9b2"
      },
      "source": [
        "---\n",
        "### ✅ Wrap‑up\n",
        "- `Ġ` marks a **leading space** for GPT‑2’s byte‑level BPE tokenizer (you’ll see it only in token strings).\n",
        "- Models consume **IDs** and **tensors**; we printed their shapes to make this concrete.\n",
        "- GPT‑2’s `last_hidden_state` provides **contextual token embeddings**.\n",
        "- Pipelines make tasks like **summarization** and **sentiment** a one‑liner for demos."
      ],
      "id": "h-xGvjWaj9b2"
    }
  ]
}